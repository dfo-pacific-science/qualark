# Qualark Data Processing Pipeline

This project takes DIDSON Sonar files and test fishing species count data and processes them for storage and analysis.

Data are collected in the field by Fraser Interior Area Stock Assessment Biologists. Those data are submitted to this data system through a secure upload page hosted on an R Shiny Server in a Linux Desktop (LxD) container. 1) When the files are uploaded, they should trigger a series of events including uploading raw data copies to SharePoint (done use Microsoft365 R package), 2) Parsing the excel files into seperate .csv files into a temp forlder on our LxD for version control purposes; 3) Data quality and validation checks; 4) Transformation to the database schema; 5) Upsert to the Postgres DB.

Comprehensive testing and notifications For any errors that occur during the proces..

This project converts functionality originally developed in Azure Data Factory (ADF) JSON scripts and Logic Apps to open-source R scripts and Azure DevOps pipelines, enabling us to escape Azure dependencies while maintaining the same functionality.

## Project Structure

```
qualark/
â”œâ”€â”€ r/                                    # R scripts (replaces ADF)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ connections.R                 # Database and storage connections
â”‚   â”œâ”€â”€ data_flows/                       # Data processing scripts
â”‚   â”‚   â”œâ”€â”€ process_didson_data_corrected.R
â”‚   â”‚   â”œâ”€â”€ process_testfishing_data_corrected.R
â”‚   â”‚   â”œâ”€â”€ sql_integration.R             # Database operations
â”‚   â”‚   â””â”€â”€ sharepoint_integration.R     # SharePoint uploads
â”‚   â”œâ”€â”€ utils/                            # Utility functions
â”‚   â”‚   â”œâ”€â”€ email_notifications.R        # Enhanced email system
â”‚   â”‚   â”œâ”€â”€ error_handling.R             # Error management
â”‚   â”‚   â”œâ”€â”€ pipeline_status.R            # Status tracking
â”‚   â”‚   â””â”€â”€ database_backup.R            # Backup procedures
â”‚   â”œâ”€â”€ parse_excel_to_csv.R             # Excel to CSV conversion utility
â”‚   â”œâ”€â”€ setup.R                          # Package installation and setup
â”‚   â””â”€â”€ main.R                           # Main orchestration script
â”œâ”€â”€ data/                                # Data storage (replaces Azure Data Lake)
â”‚   â”œâ”€â”€ bronze/                          # Raw data layers 
â”‚   â”‚   â””â”€â”€ Prototype/                   # Sample data files
â”‚   â”œâ”€â”€ csv_parsed/                      # Parsed CSV files (version controlled)
â”‚   â”‚   â”œâ”€â”€ lookup_data/                 # Lookup tables as CSV
â”‚   â”‚   â””â”€â”€ main_data/                   # Main data files as CSV
â”‚   â”œâ”€â”€ silver/                          # Processed data layer
â”‚   â”‚   â”œâ”€â”€ processed_from_csv/          # Latest processed data
â”‚   â”‚   â”œâ”€â”€ lookups_from_csv/            # Processed lookup tables
â”‚   â”‚   â”œâ”€â”€ reports/                     # Pipeline summary reports
â”‚   â”‚   â””â”€â”€ validation/                  # Data quality validation results
â”‚   â””â”€â”€ gold/                            # Analytics-ready data (future)
â”œâ”€â”€ prototype_data/                      # Original Excel files
â”‚   â”œâ”€â”€ lookup/                          # Lookup Excel files
â”‚   â”œâ”€â”€ Qualark_2023_DIDSON_Counts.xlsx
â”‚   â””â”€â”€ Qualark_2023_Test_Fishing_and_Sampling.xlsx
â”œâ”€â”€ sql/                                 # Database schema files
â”‚   â”œâ”€â”€ qualarkspeciescomp_proto_schema.sql
â”‚   â””â”€â”€ insert_lookup_info.sql
â”œâ”€â”€ docs/                                # Documentation
â”œâ”€â”€ qualark-demo.Rproj                   # R project file
â”œâ”€â”€ requirements.txt                     # R package requirements
â”œâ”€â”€ README.md                            # This file
â”œâ”€â”€ SETUP_GUIDE.md                       # Setup instructions
â”œâ”€â”€ TESTING_GUIDE.md                     # Comprehensive testing guide
â””â”€â”€ TODO_LIST.md                         # Project progress tracking
```

## Features

### âœ… **Current Capabilities**
- **Excel File Processing**: Automatically parses Excel files to CSV for version control
- **Data Validation**: Input format validation and data quality checks
- **Modular Processing**: Independent data flows for DIDSON and Test Fishing data
- **Lookup Integration**: Automatic lookup table processing and joining
- **Error Handling**: Comprehensive error logging and recovery
- **Local Development**: Works without database or email dependencies
- **Database Integration**: Complete PostgreSQL integration with flip switches
- **SharePoint Integration**: Automatic upload of raw files for provenance
- **Enhanced Notifications**: Comprehensive email notification system
- **Pipeline Status Management**: Real-time status tracking and reporting
- **Database Backup**: Automated backup and recovery procedures
- **Azure DevOps Integration**: Complete CI/CD pipeline configuration

### ðŸ”„ **Data Flow**
1. **Excel Files Uploaded through Web Page** â†’ **CSV Parsing** â†’ **SharePoint Uploa/Backup** â†’ **Data Validation** â†’ **Processing** â†’ **Silver Layer**
2. **Lookup Tables** â†’ **CSV Parsing** â†’ **Database Integration** â†’ **Data Joining**
3. **Quality Validation** â†’ **Database Insertion** â†’ **Backup Creation** â†’ **Notification** â†’ **Summary Generation**

## Quick Start

### Prerequisites
- R 4.0+ with required packages (see `requirements.txt`)
- Excel files in `prototype_data/` directory

### Installation
```r
# Install required packages
source("r/setup.R")

# Or install manually
install.packages(c("here", "logging", "dplyr", "readxl", "readr", "tidyr", "DBI", "odbc", "lubridate", "mailR", "httr", "jsonlite", "RMySQL", "RSQLite", "RPostgres"))
```

### Basic Usage

#### Run All Pipelines
```r
source("r/main.R")
results <- run_all_pipelines()
```

#### Run Individual Pipelines
```r
# Test Fishing pipeline
result <- run_pipeline("testfishing")

# DIDSON pipeline
result <- run_pipeline("didson")

# Parse Excel files only
result <- run_pipeline("parse")

# Validate input format
result <- run_pipeline("validate")
```

#### Command Line Usage
```bash
# Run all pipelines
Rscript r/main.R

# Run specific pipeline
Rscript r/main.R testfishing
Rscript r/main.R didson
Rscript r/main.R parse
Rscript r/main.R validate
Rscript r/main.R data_quality
```

## Data Processing

### Test Fishing Data
- **Input**: `prototype_data/Qualark_2023_Test_Fishing_and_Sampling.xlsx`
- **Processing**: Detailed Catch sheet â†’ Drifts + Fish Samples
- **Output**: 
  - `data/silver/processed_from_csv/drifts_processed_[timestamp].csv`
  - `data/silver/processed_from_csv/fish_samples_processed_[timestamp].csv`

### DIDSON Data
- **Input**: `prototype_data/Qualark_2023_DIDSON_Counts.xlsx`
- **Processing**: Multiple sheets â†’ Consolidated DIDSON data
- **Output**: `data/silver/processed_from_csv/didson_processed_[timestamp].csv`

### Lookup Tables
- **Input**: `prototype_data/lookup/*.xlsx`
- **Processing**: Excel â†’ CSV â†’ Integration
- **Output**: `data/silver/lookups_from_csv/*.csv`

## Configuration

### Flip Switches for Production Readiness

The pipeline includes flip switches to enable/disable production features:

```r
# Enable database operations (when ready)
source("r/data_flows/sql_integration.R")
enable_database_operations()

# Enable email notifications (when ready)
source("r/utils/email_notifications.R")
enable_email_operations()

# Enable SharePoint operations (when ready)
source("r/data_flows/sharepoint_integration.R")
enable_sharepoint_operations()

# Enable backup operations (when ready)
source("r/utils/database_backup.R")
enable_backup_operations()
```

### Database Configuration
Edit `r/config/connections.R` to configure database connections:

```r
# PostgreSQL configuration (for production)
db_config <- list(
  driver = "PostgreSQL",
  server = "localhost",
  database = "qualark_db",
  port = 5432,
  username = "your_username",
  password = Sys.getenv("DB_PASSWORD")
)
```

### Email Configuration
Configure email notifications in `r/config/connections.R`:

```r
email_config <- list(
  smtp_server = "smtp.gmail.com",
  port = 587,
  username = "your_email@gmail.com",
  password = Sys.getenv("EMAIL_PASSWORD"),
  from = "your_email@gmail.com",
  to = c("recipient1@example.com", "recipient2@example.com")
)
```

### SharePoint Configuration
Configure SharePoint integration in `r/config/connections.R`:

```r
sharepoint_config <- list(
  site_id = Sys.getenv("SHAREPOINT_SITE_ID"),
  token = Sys.getenv("SHAREPOINT_TOKEN"),
  base_url = "https://graph.microsoft.com/v1.0/sites"
)
```

## Testing

### Step-by-Step Testing Guide

1. **Setup Environment**
   ```r
   source("r/setup.R")
   ```

2. **Parse Excel Files**
   ```r
   source("r/main.R")
   result <- run_pipeline("parse")
   ```

3. **Validate Input Format**
   ```r
   result <- run_pipeline("validate")
   ```

4. **Run Test Fishing Pipeline**
   ```r
   result <- run_pipeline("testfishing")
   ```

5. **Run DIDSON Pipeline**
   ```r
   result <- run_pipeline("didson")
   ```

6. **Validate Data Quality**
   ```r
   result <- run_pipeline("data_quality")
   ```

7. **Run All Pipelines**
   ```r
   results <- run_all_pipelines()
   ```

### Expected Outputs
- CSV files in `data/csv_parsed/`
- Processed data in `data/silver/processed_from_csv/`
- Validation reports in `data/silver/validation/`
- Summary reports in `data/silver/reports/`

## Production Setup

### PostgreSQL Database
1. Install PostgreSQL on your system
2. Create database: `qualark_db`
3. Run schema: `sql/qualarkspeciescomp_proto_schema.sql`
4. Update `r/config/connections.R` with database credentials

### Email Notifications
1. Configure SMTP settings in `r/config/connections.R`
2. Set environment variables for credentials
3. Test email functionality

### Azure DevOps (Recommended)
1. Set up Azure DevOps project and import repository
2. Configure variable groups for database and email credentials
3. Set up service connections for PostgreSQL and SMTP
4. Configure pipeline triggers and schedules
5. See `azure-devops/README.md` for detailed setup instructions

### GitHub Actions (Alternative)
1. Set up repository secrets for database and email credentials
2. Enable GitHub Actions workflows
3. Configure file monitoring and scheduled runs

## Troubleshooting

### Common Issues

1. **Excel Parsing Errors**
   - Check file paths in `prototype_data/`
   - Verify Excel file format and structure
   - Review parsing logs

2. **Data Validation Failures**
   - Check column names and data types
   - Verify lookup table structure
   - Review validation reports

3. **Database Connection Issues**
   - Verify database credentials
   - Check network connectivity
   - Review connection configuration

### Logs and Reports
- **Logs**: Check R console output and log files
- **Validation**: `data/silver/validation/validation_results_*.json`
- **Reports**: `data/silver/reports/pipeline_summary_*.json`

## Project Status

### âœ… **Completed**
- [x] Excel file parsing and CSV conversion
- [x] Data validation and quality checks
- [x] Modular data processing pipelines
- [x] Error handling and recovery
- [x] Comprehensive testing suite
- [x] Documentation and guides
- [x] Local development environment
- [x] Complete SQL database integration with flip switches
- [x] SharePoint integration for raw file storage
- [x] Enhanced email notification system
- [x] Pipeline status management and reporting
- [x] Database backup and recovery procedures
- [x] Azure DevOps pipeline configuration
- [x] Production-ready flip switches

### ðŸ”„ **Ready for Production**
- [ ] Configure database credentials and enable database operations
- [ ] Configure email credentials and enable email notifications
- [ ] Configure SharePoint credentials and enable SharePoint operations
- [ ] Set up Azure DevOps project and configure pipelines
- [ ] Test complete production workflow

### ðŸ“‹ **Future Enhancements**
- [ ] Data visualization dashboard
- [ ] Advanced analytics and reporting
- [ ] API endpoints for data access
- [ ] Mobile application for field data collection

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support

For questions or issues:
1. Check the troubleshooting section
2. Review the logs and reports
3. Create an issue in the repository
4. Contact the development team